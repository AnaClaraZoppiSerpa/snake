{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importações e funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygame\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# our classes\n",
    "from agentLFA import AgentLFA, QLearningAgentLFA, MonteCarloAgentLFA\n",
    "from environment import Environment\n",
    "from screen import Screen\n",
    "\n",
    "# define environment\n",
    "ACTION_SPACE = np.eye(3)\n",
    "NUM_ACTIONS = 3\n",
    "NUM_STATES = 2 ** 11\n",
    "\n",
    "# Set options to activate or deactivate the game view, and its speed\n",
    "pygame.font.init()\n",
    "\n",
    "def plot_metrics(metrics, filepath=None):\n",
    "    formatted_dict = {'episodes': [],\n",
    "                      'metrics': [],\n",
    "                      'results': []}\n",
    "\n",
    "    n = len(metrics['episodes'])\n",
    "    for i in range(n):\n",
    "        episode = metrics['episodes'][i]\n",
    "        score = metrics['scores'][i]\n",
    "        reward = metrics['rewards'][i]\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('score')\n",
    "        formatted_dict['results'].append(score)\n",
    "\n",
    "        formatted_dict['episodes'].append(episode)\n",
    "        formatted_dict['metrics'].append('reward')\n",
    "        formatted_dict['results'].append(reward)\n",
    "\n",
    "    df_metrics = pd.DataFrame(formatted_dict)\n",
    "    sns.lineplot(data=df_metrics, x='episodes', y='results', hue='metrics')\n",
    "    if filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "\n",
    "def decode_state(encoded_state):\n",
    "    \"\"\"\n",
    "    Decode a binary representation of a state into its decimal base;\n",
    "    \n",
    "    encoded_state: an array of 0s and 1s representing a binary value\n",
    "    \n",
    "    return: decimal value\n",
    "    \"\"\"\n",
    "    decoded = ''\n",
    "    for s in encoded_state:\n",
    "        decoded += str(s)\n",
    "\n",
    "    return int(decoded, 2)\n",
    "\n",
    "def decode_action(encoded_action):\n",
    "    if isinstance(encoded_action, np.ndarray):\n",
    "        return encoded_action.argmax()\n",
    "    return encoded_action\n",
    "\n",
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -10 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def reward_euclidean(env):\n",
    "    player_pos = [env.player.x, env.player.y]\n",
    "    food_pos = [env.food.x_food, env.food.y_food]\n",
    "    dist = distance.euclidean(player_pos, food_pos)\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10000\n",
    "    elif env.player.eaten:\n",
    "        reward = 10000\n",
    "    else:\n",
    "        if dist > 100:\n",
    "            reward = (-1*dist)/10\n",
    "        if dist <= 100:\n",
    "            reward = (100-dist)/10\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1      Score: 0\n",
      "Game 2      Score: 0\n",
      "Game 3      Score: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-e963538f3566>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 118\u001B[0;31m \u001B[0mmetrics\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrun_monte_carlo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmonteCarloAgent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward_function\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreward_euclidean\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepisodes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mspeed\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    119\u001B[0m \u001B[0mend\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-e963538f3566>\u001B[0m in \u001B[0;36mrun_monte_carlo\u001B[0;34m(agent, reward_function, episodes, display, speed, verbose, first_visit)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m                 \u001B[0mscreen\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     55\u001B[0m                 \u001B[0mpygame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mspeed\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/mestrado/reinforcement_learning/snake/screen.py\u001B[0m in \u001B[0;36mdisplay\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgameDisplay\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfill\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m255\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m255\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m255\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__display_ui\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m         self.env.player.display_player(self.env.player.position[-1][0], self.env.player.position[-1][1],\n\u001B[1;32m     34\u001B[0m                                        self.env.player.food, self.env.game)\n",
      "\u001B[0;32m~/mestrado/reinforcement_learning/snake/screen.py\u001B[0m in \u001B[0;36m__display_ui\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgameDisplay\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mblit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext_highest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m190\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m440\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgameDisplay\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mblit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext_highest_number\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m360\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m440\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgameDisplay\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mblit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def run_monte_carlo(agent: AgentLFA, reward_function, episodes, display, speed, verbose=True, first_visit=False):\n",
    "    # setting random seed\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if display:\n",
    "        pygame.init()\n",
    "\n",
    "    env = Environment(440, 440, reward_function)\n",
    "    screen = Screen(env)\n",
    "\n",
    "    episode = 0\n",
    "    metrics = {'episodes': [],\n",
    "               'scores': [],\n",
    "               'rewards': []}\n",
    "\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "\n",
    "    start = time.time()\n",
    "    while episode < episodes:\n",
    "        states_and_actions_visited = []\n",
    "\n",
    "        if display:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "\n",
    "            screen.display()\n",
    "\n",
    "        state1, done = env.reset()\n",
    "        #state1 = decode_state(state1)\n",
    "        action1 = agent.choose_action(state1)\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            # Getting the next state, reward\n",
    "            state2, reward, done = env.step(action1)\n",
    "            #state2 = decode_state(state2)\n",
    "            # Choosing the next action\n",
    "            action2 = agent.choose_action(state2)\n",
    "\n",
    "            # Learning the Q-value\n",
    "            #decoded_action1 = decode_action(action1)\n",
    "            #decoded_action2 = decode_action(action2)\n",
    "            #agent.update(state1, state2, reward, decoded_action1, decoded_action2)\n",
    "\n",
    "            state1 = state2\n",
    "            action1 = action2\n",
    "            episode_reward += reward\n",
    "\n",
    "            if display:\n",
    "                screen.display()\n",
    "                pygame.time.wait(speed)\n",
    "\n",
    "            states_and_actions_visited.append((state2, action2, reward))\n",
    "\n",
    "            end = time.time()\n",
    "            diff = end - start\n",
    "            if diff > 600: # 10min\n",
    "                break\n",
    "\n",
    "        # Acabou o episódio, hora de aprender\n",
    "        G = 0\n",
    "        for i, step in enumerate(states_and_actions_visited[::-1]):\n",
    "            state, action, r = step\n",
    "            G = agent.gamma*G + r\n",
    "\n",
    "            do_update = True\n",
    "\n",
    "            if first_visit:\n",
    "                if state in [x[0] for x in states_and_actions_visited[::-1][len(states_and_actions_visited)-i:]]:\n",
    "                    do_update = False\n",
    "\n",
    "            if do_update:\n",
    "                sa_pair = (decode_state(step[0]), decode_action(step[1]))\n",
    "\n",
    "                if sa_pair in returns_sum:\n",
    "                    returns_sum[sa_pair] += G\n",
    "                else:\n",
    "                    returns_sum[sa_pair] = G\n",
    "\n",
    "                if sa_pair in returns_count:\n",
    "                    returns_count[sa_pair] += 1.0\n",
    "                else:\n",
    "                    returns_count[sa_pair] = 1\n",
    "\n",
    "                average = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "                agent.update(average, state, sa_pair[1])\n",
    "\n",
    "\n",
    "        # Incrementar episódios e ir guardando as nossas métricas\n",
    "        episode += 1\n",
    "        if verbose:\n",
    "            print(f'Game {episode}      Score: {env.game.score}')\n",
    "\n",
    "        mean_reward = episode_reward/episodes\n",
    "        metrics['episodes'].append(episode)\n",
    "        metrics['rewards'].append(mean_reward)\n",
    "        metrics['scores'].append(env.game.score)\n",
    "\n",
    "        end = time.time()\n",
    "        diff = end - start\n",
    "        if diff > 600: # 10min\n",
    "            break\n",
    "\n",
    "\n",
    "    return metrics\n",
    "\n",
    "N0 = 1\n",
    "gamma = 1\n",
    "\n",
    "# define agent\n",
    "monteCarloAgent = MonteCarloAgentLFA(N0, gamma, NUM_STATES, NUM_ACTIONS, ACTION_SPACE)\n",
    "\n",
    "start = time.time()\n",
    "metrics = run_monte_carlo(monteCarloAgent, reward_function=reward_euclidean, episodes=100, speed=0, display=True)\n",
    "end = time.time()\n",
    "\n",
    "plot_metrics(metrics, filepath=None)\n",
    "\n",
    "print('Run time:', (end-start), 'seconds')\n",
    "print('Max. Score:', max(metrics['scores']))\n",
    "print('Mean Last Scores:', np.mean(metrics['scores'][-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}