{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos do Trabalho\n",
    "O objetivo deste trabalho é aplicar métodos de aprendizado por reforço a um problema escolhido e modelado pelos estudantes, comparando os resultados obtidos pelos métodos Monte Carlo, Q-Learning e SARSA(lambda), avaliando: qualidade das soluções, custo computacional, tamanho do espaço de estados, tamanho do espaço de ações e influência da função de reforço no resultado. Também pretende-se aplicar aproximadores lineares de função a cada método e comparar o desempenho de cada método **com** e **sem** o aproximador de função.\n",
    "\n",
    "# Problema\n",
    "Escolhemos trabalhar com o jogo Snake, também conhecido como \"Serpente\" ou \"Jogo da Cobrinha\", disponível em https://www.google.com/fbx?fbx=snake_arcade.\n",
    "\n",
    "O jogador é uma cobra, inicialmente pequena, e seu objetivo é comer maçãs. A cobra pode se mover para cima, para baixo, para a esquerda e para a direita. Maçãs aparecem em posições aleatórias do mapa, e o jogador deve se movimentar a fim de passar pela posição que contém a maçã atual. O jogador pode se mover livremente, mas não pode colidir com as bordas do mapa (paredes), nem com seu próprio corpo. Conforme as maçãs são consumidas, o tamanho da cobra aumenta, e há mais risco de colidir com seu próprio corpo ao se movimentar pelo mapa, assim aumentando a dificuldade do jogo.\n",
    "\n",
    "## Formulação do problema como MDP\n",
    "\n",
    "Conforme visto em aula, um Processo Decisório de Markov (MDP) é uma tupla <S, P, A, R, gamma>, sendo S um conjunto finito de estados, A um conjunto finito de ações, P uma matriz de probabilidades de transições entre estados, R uma função de reforço e gamma um fator de desconto, que é um número real no intervalo [0,1].\n",
    "\n",
    "Um estado é uma tupla com 11 valores booleanos:\n",
    "1. Se há perigo (parede ou o próprio corpo da cobra) uma ou duas unidades do tabuleiro à frente;\n",
    "2. Se há perigo uma ou duas unidades à direita;\n",
    "3. Se há perigo uma ou duas unidades à esquerda;\n",
    "4. Se a cobra está se movendo para a esquerda;\n",
    "5. Se a cobra está se movendo para a direita;\n",
    "6. Se a cobra está se movendo para cima;\n",
    "7. Se a cobra está se movendo para baixo.\n",
    "8. Se há comida à esquerda;\n",
    "9. Se há comida à direita;\n",
    "10. Se há comida para cima;\n",
    "11. Se há comida para baixo;\n",
    "\n",
    "Essas direções (esquerda, direita, cima, baixo) são sempre consideradas na perspectiva da cabeça da cobra.\n",
    "\n",
    "As ações possíveis são:\n",
    "- Continuar indo na mesma direção, representada pelo vetor [1,0,0];\n",
    "- Virar para a direita, representada por [0,1,0];\n",
    "- Virar para a esquerda, representada por [0,0,1];\n",
    "\n",
    "A tela de jogo tem 440 pixels de largura e 440 pixels de altura. Cada parte do corpo da cobra é um quadrado de 20 pixels de altura e 20 pixels de largura. A maçã tem 20 pixels de altura e 20 pixels de largura também.\n",
    "\n",
    "A matriz de transição de probabilidades P contém somente uns e zeros, pois dada uma posição (estado) atual no tabuleiro e uma ação, o resultado é sempre o mesmo. Para cada estado atual **s**, estado resultante **s'** quando tomada uma ação **a** e estados restantes **Z** que não são obtidos tomando **a** a partir de **s**, a probabilidade de ir de **s** para **s'** é 1, enquanto que de **s** a qualquer dos **Z** é zero. Isso vale para todos os estados e ações, por isso a matriz terá somente zeros e uns.\n",
    "\n",
    "Quanto à função de reforço, definimos uma função padrão e depois exploramos outras. Isso será detalhado mais adiante no relatório, assim como os diferentes fatores de desconto (gamma) adotados nos experimentos.\n",
    "\n",
    "## Natureza do ambiente\n",
    "\n",
    "* Determinístico ou estocástico: **determinístico**, pois, dado um estado corrente e uma ação, o estado resultante é sempre o mesmo (dada uma posição, a ação de continuar na mesma direção/virar para a direita/virar para a esquerda leva sempre ao mesmo destino).\n",
    "\n",
    "* Contínuo ou discreto: **discreto**, já que temos um conjunto finito de ações que podemos escolher tomar a partir de um certo estado (virar para a direita, virar para a esquerda, continuar na mesma direção).\n",
    "\n",
    "* Episódico ou não episódico: **episódico**, sendo cada episódio um jogo. O episódio termina somente quando a cobra morre.\n",
    "\n",
    "* Single-agent ou multi-agent: **single-agent**, pois nosso único agente é a cobra.\n",
    "\n",
    "## Modelo de discretização adotado\n",
    "\n",
    "Como nosso ambiente já é discreto, não precisamos adotar nenhuma estratégia de discretização.\n",
    "\n",
    "## Estados terminais\n",
    "\n",
    "Nossos estados terminais são aqueles referentes à morte da cobra, portanto ocorrem somente auando a cobra colide com a parede e quando a cobra colide com seu próprio corpo.\n",
    "\n",
    "## Funções de Reforço\n",
    "\n",
    "Nossa **função de reforço padrão** foi -10 quando a cobra morre, +10 quando come a maçã e 0 caso contrário.\n",
    "\n",
    "```\n",
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -10 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward\n",
    "```\n",
    "\n",
    "Exploramos também as seguintes:\n",
    "\n",
    "* Função reward1: visa penalizar a cobra por não estar comendo a maçã.\n",
    "\n",
    "```\n",
    "def reward1(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        -1 otherwise\n",
    "    \"\"\"\n",
    "    reward = -1\n",
    "    if env.game.crash:\n",
    "        reward = -100\n",
    "    elif env.player.eaten:\n",
    "        reward = 100\n",
    "\n",
    "    return reward\n",
    "```\n",
    "\n",
    "* Função reward_linear: visa penalizar a cobra por cada momento em que não melhorar seu desempenho. É possível fornecer diferentes penalizações (penalty_rate).\n",
    "\n",
    "```\n",
    "def reward_linear(env, penalty_rate=0.01, dist_metric=cityblock):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -100 when Snake crashes.\n",
    "        +100 when Snake eats food\n",
    "        -0.01*non_improvement otherwise\n",
    "    \"\"\"\n",
    "    global non_improvement\n",
    "    reward = -non_improvement*penalty_rate\n",
    "    if env.game.crash:\n",
    "        reward = -100\n",
    "    elif env.player.eaten:\n",
    "        reward = 100\n",
    "        non_improvement = 0\n",
    "    else:\n",
    "        non_improvement += 1\n",
    "\n",
    "    return reward\n",
    "```\n",
    "\n",
    "## Parâmetros\n",
    "\n",
    "Ao longo dos experimentos, diferentes valores para os seguintes parâmetros foram explorados:\n",
    "\n",
    "* Fator de desconto (gamma): o fator de desconto \n",
    "\n",
    "* Step-size (alfa):\n",
    "\n",
    "* N0:\n",
    "\n",
    "# Implementação\n",
    "\n",
    "## Como o problema foi modelado em Python\n",
    "\n",
    "## Particularidades e restrições da implementação\n",
    "\n",
    "Para executar o código, é preciso instalar a biblioteca PyGame. Isso pode ser feito com o comando `pip install pygame` no terminal.\n",
    "\n",
    "# Métodos\n",
    "\n",
    "## Monte Carlo\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "## SARSA\n",
    "\n",
    "## Aproximador Linear de Função\n",
    "\n",
    "# Experimentos realizados\n",
    "\n",
    "Com o método Q-Learning, experimentamos diferentes valores de N0 e gamma. Depois, mantendo N0 = ... e gamma = ..., experimentamos diferentes funções de reforço.\n",
    "\n",
    "- colocar tabelas\n",
    "\n",
    "Com o método Monte Carlo, ... .\n",
    "\n",
    "# Discussão de Resultados\n",
    "- Qual método demorou menos?\n",
    "- Qual método fez a cobra sobreviver mais tempo?\n",
    "- Quais parâmetros foram melhores?\n",
    "- etc...\n",
    "\n",
    "# Membros do Grupo e Contribuições\n",
    "### Ana Clara Zoppi Serpa (RA 165880)\n",
    "- Implementação do método Monte Carlo\n",
    "- Implementação do método SARSA(lambda)\n",
    "- Escrita do relatório\n",
    "- Formulação do problema como MDP\n",
    "\n",
    "### Gabriel Oliveira dos Santos (RA 197460)\n",
    "- Implementação do método Q-Learning\n",
    "- Implementação do método SARSA(0)\n",
    "- Experimentos com diferentes funções de reforço, nos métodos Q-Learning e SARSA(0)\n",
    "- Formulação do problema como MDP\n",
    "\n",
    "### Silvio Bento Garcia Junior (RA 265194)\n",
    "- Melhorias nos experimentos com o método de Monte Carlo, explorando diferentes funções de reforço\n",
    "- Formulação do problema como MDP\n",
    "\n",
    "### Tito Barbosa Rezende (RA 025327)\n",
    "- Implementação e correção de problemas no método Monte Carlo\n",
    "- Implementação do Aproximador Linear de Função para o método Monte Carlo\n",
    "- Formulação do problema como MDP\n",
    "\n",
    "# Observações\n",
    "\n",
    "Na próxima seção, colocamos o código-fonte da implementação. Ao executá-los, uma janela com o jogo será aberta, mas ficará preta pois desabilitamos os recursos gráficos a fim de realizar medições mais rápidas dos dados que precisamos. Optamos por adicionar uma seção de demonstração, logo abaixo da seção de código-fonte, com os recursos gráficos habilitados para que seja possível ver o aprendizado da cobra no jogo.\n",
    "\n",
    "Se desejar ver as animações dos experimentos, apesar de demorarem, basta alterar o parâmetro display das chamadas a run_q_learning, run_monte_carlo e run_sarsa para True.\n",
    "\n",
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstração do jogo\n",
    "\n",
    "Nessa seção, colocamos algumas células que executam o jogo mostrando graficamente o aprendizado da cobra ao longo das iterações, já que para os experimentos anteriores desabilitamos os recursos gráficos a fim de realizar medições mais rápidas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
