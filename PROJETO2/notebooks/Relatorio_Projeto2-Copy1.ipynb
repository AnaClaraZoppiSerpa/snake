{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coisas que falta escrever:\n",
    "\n",
    "characteristics of the selected algorithms\n",
    "\n",
    "parameters employed in the methods\n",
    "\n",
    "how the chosen methods work with the nature of the action-space representation\n",
    "\n",
    "on-policy x off-policy results\n",
    "\n",
    "did you achieve the policy?\n",
    "\n",
    "non-linear function approximation advantages and disadvantages in relation to project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos do Trabalho\n",
    "O objetivo deste trabalho é aplicar métodos de aprendizado por reforço ao problema Snake, sendo estes métodos um on-policy e um off-policy, a fim de avaliá-los quanto à qualidade das soluções encontradas e custo computacional. Também pretende-se comparar os resultados obtidos com estes novos métodos aos resultados obtidos previamente no Projeto 1.\n",
    "\n",
    "Para o projeto 2, mantivemos o mesmo problema do projeto 1 (o jogo Snake) e a mesma formulação MDP. Assim, a natureza do ambiente e os estados terminais são os mesmos do projeto 1. Também mantivemos as mesmas funções de reforço experimentadas no projeto 1. Dessa forma, o que muda em relação ao projeto 1 são os algoritmos empregados para resolver o problema. Realizamos experimentos com os métodos DQN (Deep Q-Learning) e A2C (Advantage Actor-Critic).\n",
    "\n",
    "# Membros do Grupo e Contribuições\n",
    "\n",
    "### Ana Clara Zoppi Serpa (RA 165880)\n",
    "- Experimentos com DQN\n",
    "- Análise de resultados experimentais\n",
    "- Escrita do relatório ()\n",
    "- Gravação do vídeo\n",
    "\n",
    "### Gabriel Oliveira dos Santos (RA 197460)\n",
    "- Experimentos com A2C\n",
    "- Análise de resultados experimentais\n",
    "- Escrita do relatório (explicação do A2C)\n",
    "- Adaptação do nosso embiente para usar a biblioteca Stable Baselines\n",
    "\n",
    "### Silvio Bento Garcia Junior (RA 265194)\n",
    "- Experimentos com PPO\n",
    "- Escrita do relatório (explicação do DQN)\n",
    "- Análise de resultados experimentais\n",
    "\n",
    "### Tito Barbosa Rezende (RA 025327)\n",
    "- Experimentos com DQN\n",
    "- Escrita do relatório (explicação do DQN)\n",
    "- Análise de resultados experimentais\n",
    "\n",
    "### Matteo Di Fabio (RA 264339)\n",
    "- Experimentos com A2C\n",
    "- Adaptação do nosso embiente para usar a biblioteca Stable Baselines\n",
    "- Análise de resultados experimentais\n",
    "\n",
    "## Link para o vídeo\n",
    "\n",
    "# Problema\n",
    "Escolhemos trabalhar com o jogo Snake, também conhecido como \"Serpente\" ou \"Jogo da Cobrinha\", disponível em https://www.google.com/fbx?fbx=snake_arcade.\n",
    "\n",
    "O jogador é uma cobra, inicialmente pequena, e seu objetivo é comer maçãs. A cobra pode se mover para cima, para baixo, para a esquerda e para a direita. Maçãs aparecem em posições aleatórias do mapa, e o jogador deve se movimentar a fim de passar pela posição que contém a maçã atual. O jogador pode se mover livremente, mas não pode colidir com as bordas do mapa (paredes), nem com seu próprio corpo. Conforme as maçãs são consumidas, o tamanho da cobra aumenta, e há mais risco de colidir com seu próprio corpo ao se movimentar pelo mapa, assim aumentando a dificuldade do jogo.\n",
    "\n",
    "## Formulação do problema como MDP\n",
    "\n",
    "Conforme visto em aula, um Processo Decisório de Markov (MDP) é uma tupla <S, P, A, R, gamma>, sendo S um conjunto finito de estados, A um conjunto finito de ações, P uma matriz de probabilidades de transições entre estados, R uma função de reforço e gamma um fator de desconto, que é um número real no intervalo [0,1].\n",
    "\n",
    "Um estado é uma tupla com 11 valores booleanos:\n",
    "1. Se há perigo (parede ou o próprio corpo da cobra) uma ou duas unidades do tabuleiro à frente;\n",
    "2. Se há perigo uma ou duas unidades à direita;\n",
    "3. Se há perigo uma ou duas unidades à esquerda;\n",
    "4. Se a cobra está se movendo para a esquerda;\n",
    "5. Se a cobra está se movendo para a direita;\n",
    "6. Se a cobra está se movendo para cima;\n",
    "7. Se a cobra está se movendo para baixo.\n",
    "8. Se há comida à esquerda;\n",
    "9. Se há comida à direita;\n",
    "10. Se há comida para cima;\n",
    "11. Se há comida para baixo;\n",
    "\n",
    "Há 2^11 estados possíveis.\n",
    "\n",
    "Essas direções (esquerda, direita, cima, baixo) são sempre consideradas na perspectiva da cabeça da cobra.\n",
    "\n",
    "As ações possíveis são:\n",
    "- Continuar indo na mesma direção, representada pelo vetor [1,0,0];\n",
    "- Virar para a direita, representada por [0,1,0];\n",
    "- Virar para a esquerda, representada por [0,0,1];\n",
    "\n",
    "A tela de jogo tem 440 pixels de largura e 440 pixels de altura. Cada parte do corpo da cobra é um quadrado de 20 pixels de altura e 20 pixels de largura. A maçã tem 20 pixels de altura e 20 pixels de largura também.\n",
    "\n",
    "A matriz de transição de probabilidades P contém somente uns e zeros, pois dada uma posição (estado) atual no tabuleiro e uma ação, o resultado é sempre o mesmo. Para cada estado atual **s**, estado resultante **s'** quando tomada uma ação **a** e estados restantes **Z** que não são obtidos tomando **a** a partir de **s**, a probabilidade de ir de **s** para **s'** é 1, enquanto que de **s** a qualquer dos **Z** é zero. Isso vale para todos os estados e ações, por isso a matriz terá somente zeros e uns.\n",
    "\n",
    "Quanto à função de reforço, definimos uma função padrão e depois exploramos outras. Isso será detalhado mais adiante no relatório, assim como os diferentes fatores de desconto (gamma) adotados nos experimentos.\n",
    "\n",
    "## Natureza do ambiente\n",
    "\n",
    "* Determinístico ou estocástico: **determinístico**, pois, dado um estado corrente e uma ação, o estado resultante é sempre o mesmo (dada uma posição, a ação de continuar na mesma direção/virar para a direita/virar para a esquerda leva sempre ao mesmo destino).\n",
    "\n",
    "* Contínuo ou discreto: **discreto**, já que temos um conjunto finito de ações que podemos escolher tomar a partir de um certo estado (virar para a direita, virar para a esquerda, continuar na mesma direção).\n",
    "\n",
    "* Episódico ou não episódico: **episódico**, sendo cada episódio um jogo. O episódio termina somente quando a cobra morre.\n",
    "\n",
    "* Single-agent ou multi-agent: **single-agent**, pois nosso único agente é a cobra.\n",
    "\n",
    "## Modelo de discretização adotado\n",
    "\n",
    "Como nosso ambiente já é discreto, não precisamos adotar nenhuma estratégia de discretização.\n",
    "\n",
    "## Estados Terminais\n",
    "\n",
    "Nossos estados terminais são aqueles referentes à morte da cobra, portanto ocorrem somente auando a cobra colide com a parede e quando a cobra colide com seu próprio corpo.\n",
    "\n",
    "## Funções de Reforço\n",
    "\n",
    "Nossa **função de reforço padrão** foi -50 quando a cobra morre, +10 quando come a maçã e 0 caso contrário.\n",
    "\n",
    "```\n",
    "def default_reward(env):\n",
    "    \"\"\"\n",
    "    Return the reward.\n",
    "    The reward is:\n",
    "        -50 when Snake crashes.\n",
    "        +10 when Snake eats food\n",
    "        0 otherwise\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    if env.game.crash:\n",
    "        reward = -10\n",
    "    elif env.player.eaten:\n",
    "        reward = 10\n",
    "\n",
    "    return reward\n",
    "```\n",
    "No projeto 1, experimentamos outras funções de reforço. No entanto, para os experimentos do projeto 2, decidimos manter sempre a `default_reward`, tendo em vista que apresentou bons resultados no projeto 1.\n",
    "\n",
    "# Pontos-chave da implementação\n",
    "\n",
    "- falta escrever com uns pedaços importantes do código aqui\n",
    "\n",
    "## Particularidades e restrições da implementação\n",
    "\n",
    "Para executar o código, é preciso instalar a biblioteca PyGame. Isso pode ser feito com o comando `pip install pygame` no terminal. Também é preciso instalar a biblioteca Stable Baselines 3. Isso pode ser feito com o comando `pip install stable-baselines3`.\n",
    "\n",
    "# Métodos\n",
    "\n",
    "## DQN (Deep Q-Learning)\n",
    "\n",
    "### Ideia geral do método\n",
    "\n",
    "O aprendizado por reforço pode ser suficientemente aplicável ao ambiente onde todos os estados alcançáveis podem ser gerenciados e armazenados na memória RAM padrão do computador. No entanto, no ambiente em que o número de estados supera a capacidade dos computadores, a abordagem padrão de RL não é muito aplicável. Além disso, em ambiente real, o agente tem que enfrentar problemas de estados contínuos, variáveis contínuas e de controle contínuo.\n",
    "\n",
    "Tendo em mente a complexidade do ambiente em que o agente deve operar, a tabela Q de estados ações usada no RL é substituída por uma Rede Neural Profunda que mapeia o ambiente (aproximação não linear) para ações do agente. A arquitetura da rede, a escolha dos hiperparâmetros da rede e o aprendizado são realizados durante a fase de treinamento. O DQN permite que o agente explore ambientes não estruturados e adquira conhecimentos que, com o tempo, possibilitem a imitação do comportamento humano.\n",
    "\n",
    "Durante o processo de treinamento, o agente interage com o ambiente e recebe dados, que são utilizados durante o aprendizado da Q-network. O agente explora o ambiente para construir uma imagem completa das transições e resultados da ação. No início, o agente decide sobre as ações de forma aleatória. Enquanto explora o ambiente, o agente tenta olhar para a rede Q (aproximação) para decidir como agir.\n",
    "\n",
    "É utilizado um método epsilon-guloso. Para tornar o processo de treinamento mais estável, aplica-se um buffer de replay que memoriza experiências do comportamento do agente. Em seguida, o treinamento é realizado em amostras aleatórias do buffer.\n",
    "\n",
    "### Pseudocódigo\n",
    "![](DQN.png)\n",
    "\n",
    "### Passos do Algoritimo\n",
    "1. Inicializar o buffer de replay.\n",
    "\n",
    "2. Pré-processar o ambiente e passe o estado S como entrada para a rede neural, que retornará os valores Q para cada ação possível a partir do estado S.\n",
    "\n",
    "3. Selecionar uma ação com a política epsilon-greedy, ou seja: com probabilidade epsion, selecionamos uma ação aleatória A e, com probabilidade 1-epsilon, selecionamos uma ação A para a qual Q é máximo.\n",
    "\n",
    "4. Tendo escolhido A, o agente realiza essa ação a partir de S para chegar em S' e receber uma recompensa R.\n",
    "\n",
    "5. Guardar essa transição no buffer de replay (<S,A,R,S’>).\n",
    "\n",
    "6. Amostrar alguns batches aleatórios de transições do replay buffer e calcular a loss.\n",
    "\n",
    "7. Descida do gradiente para minimizar loss da rede neural.\n",
    "\n",
    "8. Após k passos, atualizar os pesos da rede alvo com os pesos da rede real.\n",
    "\n",
    "9. Realizar esses passos por M episódios. \n",
    "\n",
    "### Parâmetros\n",
    "\n",
    "A biblioteca Stable Baselines (https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) permite alterar diversos parâmetros do método DQN. Neste trabalho, exploramos:\n",
    "\n",
    "- **total_timesteps**: o número total de amostras, ou seja, de passos do ambiente (\"environment steps\") que serão usados para o treinamento.\n",
    "- **learning_starts**: a partir de qual passo o aprendizado deve começar. Em outras palavras, são coletadas learning_steps transições (passos) do modelo antes de começar de fato o treinamento.\n",
    "- **gamma**: fator de desconto para recompensas futuras. Este fator regula o tipo de atitude do agente em relação a recompensas de longo prazo - ele pode priorizar recompensas futuras (quanto mais próximo de 1 gamma estiver) ou priorizar recompensas imediatas (quanto mais próximo de 0 gamma estiver).\n",
    "- **exploration_fraction**: fração do período de treinamento na qual a taxa de exploração, i.e, escolha de ações aleatórias, é diminuída.\n",
    "- **learning_rate**: taxa de aprendizado.\n",
    "- **batch_size**: tamanho do batch para cada atualização de gradiente.\n",
    "\n",
    "\n",
    "## A2C (Advantage Actor-Critic)\n",
    "\n",
    "### Parâmetros\n",
    "\n",
    "A biblioteca Stable Baselines (https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) permite alterar diversos parâmetros do método A2C. Neste trabalho, exploramos:\n",
    "\n",
    "- **total_timesteps**: o número total de amostras, ou seja, de passos do ambiente (\"environment steps\") que serão usados para o treinamento.\n",
    "- **gamma**: fator de desconto para recompensas futuras. Este fator regula o tipo de atitude do agente em relação a recompensas de longo prazo - ele pode priorizar recompensas futuras (quanto mais próximo de 1 gamma estiver) ou priorizar recompensas imediatas (quanto mais próximo de 0 gamma estiver).\n",
    "- **learning_rate**: taxa de aprendizado.\n",
    "- **normalize_advantage**: normalizar (true) ou não (false) a vantagem.\n",
    "- **ent_coef**: coeficiente de entropia para o cálculo da loss.\n",
    "- **vf_coef**: coeficiente da função valor para o cálculo da loss.\n",
    "- **use_rms_prop**: utilizar o otimizador RMSprop (true) ou utilizar o otimizador Adam (false).\n",
    "- **n_eval_episode**: número de episódios para avaliar o agente.\n",
    "\n",
    "\n",
    "# Discussão de Resultados\n",
    "\n",
    "## Método DQN e seus parâmetros\n",
    "\n",
    "## Método A2C e seus parâmetros\n",
    "\n",
    "## Comparando DQN e A2C\n",
    "\n",
    "## Comparando DQN com resultados do projeto 1\n",
    "\n",
    "## Comparando A2C com resultados do projeto 1\n",
    "\n",
    "## Principais Conclusões\n",
    "\n",
    "# Observações\n",
    "\n",
    "## Notebooks auxiliares, código completo e planilha de experimentos\n",
    "\n",
    "Aqui neste notebook, colocamos exemplos de código para os principais experimentos. No entanto, todos os nossos experimentos estão em notebooks auxiliares usados por cada membro do grupo.\n",
    "\n",
    "- Notebook auxiliar 1 - experimentos com DQN realizados por Ana Clara Zoppi Serpa: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/ana_DQN.ipynb\n",
    "- Notebook auxiliar 2 - experimentos com A2C realizados por Gabriel Oliveira dos Santos: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/gabriel.ipynb\n",
    "- Notebook auxiliar 3 - experimentos com DQN realizados por Tito Barbosa Rezende: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/tito_DQN.ipynb\n",
    "- Notebook auxiliar 4 - experimentos com A2C realizados por Matteo Di Fabio: https://github.com/AnaClaraZoppiSerpa/snake/blob/main/PROJETO2/notebooks/Matteo.ipynb\n",
    "\n",
    "Além disso, todos os resultados estão disponíveis na seguinte planilha: https://docs.google.com/spreadsheets/d/1MHTG_k1Mc4TGba2h3Fi95k1qa0V8B3bf4FI8xGXIzMg/edit?usp=sharing\n",
    "\n",
    "O código completo está disponível no GitHub: https://github.com/AnaClaraZoppiSerpa/snake\n",
    "\n",
    "## Outras\n",
    "\n",
    "Tentamos realizar experimentos com o método PPO (Proximal Policy Optimization), mas não obtivemos bons resultados.\n",
    "\n",
    "\n",
    "# Referências\n",
    "\n",
    "- Aulas da professora, disponibilizadas no Google Classroom da disciplina.\n",
    "\n",
    "- Livro Reinforcement Learning: An Introduction, Sutton and Barto, 2a Edição.\n",
    "\n",
    "- Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International conference on machine learning. PMLR, 2016.\n",
    "\n",
    "- Documentação da biblioteca Stable Baselines, método DQN: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "\n",
    "- Documentação da biblioteca Stable Baselines, método A2C: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html\n",
    "\n",
    "- Explicação sobre Deep Q-Learning: https://en.m.wikipedia.org/wiki/Q-learning#Deep_Q-learning\n",
    "\n",
    "- Artigo sobre métodos actor-critic: https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f\n",
    "\n",
    "- Mais sobre Deep Q-Learning: https://medium.com/@markus.x.buchholz/deep-reinforcement-learning-introduction-deep-q-network-dqn-algorithm-fb74bf4d6862\n",
    "\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529–533 (2015). https://doi.org/10.1038/nature14236\n",
    "\n",
    "### Artigos e exemplos de implementação\n",
    "\n",
    "- Artigo sobre Snake com Deep Q-Learning: https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a\n",
    "\n",
    "- Repositório do GitHub com Snake e Deep Q-Learning, no qual nos baseamos inicialmente para realizar o projeto: https://github.com/maurock/snake-ga\n",
    "\n",
    "- Exemplo de implementação do SARSA: https://www.geeksforgeeks.org/expected-sarsa-in-reinforcement-learning/\n",
    "\n",
    "- Exemplo de implementação de Environment com Gym: https://github.com/pedrohbtp/snake-rl/blob/master/snake_rl.ipynb\n",
    "\n",
    "- Tutorial de Reinforcement Learning: https://araffin.github.io/slides/rl-tuto-jnrr19/#/4\n",
    "\n",
    "- Artigo sobre SARSA(lambda) para vários jogos, inclusive Snake: http://cs229.stanford.edu/proj2012/JohnsonRobertsFisher-LearningToPlay2DVideoGames.pdf\n",
    "\n",
    "- Exemplo de implementação de Aproximador de Função: https://github.com/metastableB/Naagin-Naggin/blob/master/dlsnake/agents/approxQAgent.py\n",
    "\n",
    "- Exemplos de implementações de SARSA e Monte Carlo: https://github.com/ralhadeff/machine-learning-tools/tree/master/ReinforcementLearning\n",
    "\n",
    "- Exemplos de implementações baseados no livro Reinforcement Learning: An Introduction: https://github.com/flyywh/reinforcement-learning-1\n",
    "\n",
    "- Artigos sobre Snake, Q-Learning e SARSA: https://dkdennis.xyz/static/Nagging-report.pdf, http://cs229.stanford.edu/proj2016spr/report/060.pdf\n",
    "\n",
    "### Outras páginas\n",
    "\n",
    "- https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b\n",
    "\n",
    "- https://towardsdatascience.com/slitherin-solving-the-classic-game-of-snake-with-ai-part-2-general-purpose-random-monte-25dc0dd4c4cf\n",
    "\n",
    "- https://towardsdatascience.com/function-approximation-in-reinforcement-learning-85a4864d566\n",
    "\n",
    "- https://towardsdatascience.com/monte-carlo-learning-b83f75233f92\n",
    "\n",
    "- https://medium.com/reinforcement-learning-a-step-by-step-implementati/reinforcement-learning-a-step-by-step-implementation-using-sarsa-1cfd3e64775a\n",
    "\n",
    "- https://www.programmersought.com/article/94202345056/\n",
    "\n",
    "- https://www.youtube.com/watch?v=l0sFUU7vScA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
